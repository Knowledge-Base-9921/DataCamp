{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d3a9f8",
   "metadata": {},
   "source": [
    "# 1. Introduction to Data Pipelines\n",
    "\n",
    "Get ready to discover how data is collected, processed, and moved using data pipelines. You will explore the qualities of the best data pipelines, and prepare to design and build your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103bc882",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eba22b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas SQL\n",
    "import pandasql as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d63501",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88077269",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_csv_path = \"../Datasets/raw_data.csv\"\n",
    "raw_data = pd.read_csv(raw_data_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d965f",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52826011",
   "metadata": {},
   "source": [
    "## 1. Running an ETL Pipeline\n",
    "\n",
    "### Description\n",
    "\n",
    "Ready to run your first ETL pipeline? Let's get to it!\n",
    "\n",
    "Here, the functions ``extract()``, ``transform()``, and ``load()`` have been defined for you. To run this data ETL pipeline, you're going to execute each of these functions. If you're curious, take a peek at what the ``extract()`` function looks like.\n",
    "\n",
    "```py\n",
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}\")\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "def transform(data_frame):\n",
    "    print(f\"Transforming {data_frame.shape[0]} rows of raw data.\")\n",
    "\n",
    "def load(data_frame, target_table):\n",
    "    print(f\"Loading cleaned data to {target_table}.\")\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Use the ``extract()`` function to extract data from the ``raw_data.csv`` file.\n",
    "* Transform the ``extracted_data`` DataFrame using the ``transform()`` function.\n",
    "* Finally, load the ``transformed_data`` DataFrame to the ``cleaned_data`` SQL table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e5281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}\")\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54648450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data_frame):\n",
    "    print(f\"Transforming {data_frame.shape[0]} rows of raw data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9cf169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, target_table):\n",
    "    print(f\"Loading cleaned data to {target_table}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9600a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from ../Datasets/raw_data.csv\n",
      "Transforming 96 rows of raw data.\n",
      "Loading cleaned data to cleaned_data.\n"
     ]
    }
   ],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "extracted_data = extract(file_name=raw_data_csv_path)\n",
    "\n",
    "# Transform the extracted_data\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load the transformed_data to cleaned_data.csv\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765bfac",
   "metadata": {},
   "source": [
    "Look at that! Using the ``extract()``, ``transform()``, and ``load()`` functions, you were able to run your first ETL pipeline. Congrats!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bde02",
   "metadata": {},
   "source": [
    "## 2. ELT in Action\n",
    "\n",
    "### Description\n",
    "\n",
    "Feeling pretty good about running ETL processes? Well, it's time to give ELT pipelines a try. Like before, the ``extract()``, ``load()``, and ``transform()`` functions have been defined for you; all you'll have to worry about is running these functions. Good luck!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Use the appropriate ETL function to extract data from the ``raw_data.csv`` file.\n",
    "* Load the ``raw_data`` DataFrame into the ``raw_data`` table in a data warehouse.\n",
    "* Call the ``transform()`` function to transform the data in the ``raw_data`` source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fded870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockDataWarehouse:\n",
    "    def __init__(self):\n",
    "        self.tables = {}\n",
    "\n",
    "    @classmethod\n",
    "    def load_table(self, raw_data, table_name):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def run_sql(self, query):\n",
    "        print(f\"Transformed data with the query: {query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80a0eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}.\")\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "def transform(source_table, target_table):\n",
    "    data_warehouse.run_sql(f\"\"\"\\n\\tCREATE TABLE {target_table} AS\\n      SELECT\\n          CONCAT(\"Product ID: \", product_id),\\n          quantity * price\\n      FROM {source_table};\\n  \"\"\")\n",
    "\n",
    "def load(data_frame, table_name):\n",
    "    print(f\"Loading cleaned data to {table_name}.\")\n",
    "    data_warehouse.load_table(data_frame, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfc96ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_warehouse = MockDataWarehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f51dfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from ../Datasets/raw_data.csv.\n",
      "Loading cleaned data to raw_data.\n",
      "Transformed data with the query: \n",
      "\tCREATE TABLE cleaned_data AS\n",
      "      SELECT\n",
      "          CONCAT(\"Product ID: \", product_id),\n",
      "          quantity * price\n",
      "      FROM raw_data;\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "raw_data = extract(file_name=raw_data_csv_path)\n",
    "\n",
    "# Load the extracted_data to the raw_data table\n",
    "load(data_frame=raw_data, table_name=\"raw_data\")\n",
    "\n",
    "# Transform data in the raw_data table\n",
    "transform(\n",
    "  source_table=\"raw_data\", \n",
    "  target_table=\"cleaned_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434babfd",
   "metadata": {},
   "source": [
    "Just like that, you've successfully run your first ELT pipeline. Keep up the good work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f119ab",
   "metadata": {},
   "source": [
    "## 3. ETL and ELT Pipelines\n",
    "\n",
    "### Description\n",
    "\n",
    "Select each of the statements about ETL and ELT pipelines that are true.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Answer the question\n",
    "\n",
    "### Answers\n",
    "\n",
    "* Processes that extract, transform then load data are known as ETL pipelines.\n",
    "* ELT processes are typically found in a data platform that leverages a data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98315e",
   "metadata": {},
   "source": [
    "Fantastic! In the course, you'll predominantly spend time building ETL pipelines, but it's important to understand how (and why) ELT pipelines are being used in the modern data stack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7d23d",
   "metadata": {},
   "source": [
    "## 4. Building an ETL Pipeline\n",
    "\n",
    "### Description\n",
    "\n",
    "Ready to ratchet up the fun? In this exercise, you'll be responsible for building the rest of the ``load()`` function before running each step in the ETL process. The ``extract()`` and ``transform()`` functions have been defined for you. Good luck!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Complete the ``load()`` function by writing the ``transformed_data`` DataFrame to a ``.csv`` file, using ``file_name``.\n",
    "* Use the ``transform()`` function to clean the ``extracted_data`` DataFrame.\n",
    "* Load ``transformed_data`` to the ``transformed_data.csv`` file using the ``load()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24351a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data_frame):\n",
    "    print(\"Transformed the raw data returned from extract()\")\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8909cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from ../Datasets/raw_data.csv.\n",
      "Transformed the raw data returned from extract()\n",
      "Successfully loaded data to transformed_data.csv\n"
     ]
    }
   ],
   "source": [
    "def load(data_frame, file_name):\n",
    "  # Write cleaned_data to a CSV using file_name\n",
    "  data_frame.to_csv(file_name)\n",
    "  print(f\"Successfully loaded data to {file_name}\")\n",
    "\n",
    "extracted_data = extract(file_name=raw_data_csv_path)\n",
    "\n",
    "# Transform extracted_data using transform() function\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load transformed_data to the file transformed_data.csv\n",
    "load(data_frame=transformed_data, file_name=\"transformed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae244e64",
   "metadata": {},
   "source": [
    "See, building an ETL pipeline doesn't need to be tricky! Using Python functions, it's easy to define and execute logic to extract, transform, and load data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2330fc",
   "metadata": {},
   "source": [
    "## 5. The \"T\" in ELT\n",
    "\n",
    "### Description\n",
    "\n",
    "Let's not forget about ELT! Here, the ``extract()`` and ``load()`` functions have been defined for you. Now, all that's left is to finish defining the transform() function and run the pipeline. Go get 'em!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Update the ``transform()`` function to call the ``.execute()`` method on the ``data_warehouse`` object.\n",
    "* Use the newly-updated ``transform()`` function to populate data in the ``total_sales`` target table by transforming data in the ``raw_sales_data`` source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "920df032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, table_name):\n",
    "    print(\"Loading extracted data to sale_items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "829d280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockDataWarehouse:\n",
    "    def __init__(self):\n",
    "        self.tables = {}\n",
    "\n",
    "    @classmethod\n",
    "    def load_table(self, raw_data, table_name):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def run_sql(self, query):\n",
    "        print(f\"Transformed data with the query: {query}\")\n",
    "\n",
    "    def execute(self, query):\n",
    "        print(f\"Ran the query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd888204",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_warehouse = MockDataWarehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eee620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from ../Datasets/raw_data.csv.\n",
      "Loading extracted data to sale_items.\n",
      "Ran the query: \n",
      "  CREATE TABLE total_sales AS\n",
      "      SELECT\n",
      "          CONCAT(\"Product ID: \", product_id),\n",
      "          quantity * price\n",
      "      FROM raw_sales_data;\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Complete building the transform() function\n",
    "def transform(source_table, target_table):\n",
    "  data_warehouse.execute(f\"\"\"\n",
    "  CREATE TABLE {target_table} AS\n",
    "      SELECT\n",
    "          CONCAT(\"Product ID: \", product_id),\n",
    "          quantity * price\n",
    "      FROM {source_table};\n",
    "  \"\"\")\n",
    "\n",
    "extracted_data = extract(file_name=raw_data_csv_path)\n",
    "load(data_frame=extracted_data, table_name=\"raw_sales_data\")\n",
    "\n",
    "# Populate total_sales by transforming raw_sales_data\n",
    "transform(source_table=\"raw_sales_data\", target_table=\"total_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108bee3",
   "metadata": {},
   "source": [
    "You crushed it! Like with ETL, building an ELT pipeline can be as easy as defining and execution Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632c866",
   "metadata": {},
   "source": [
    "## 6. Extracting, Transforming, and Loading Student Scores Data\n",
    "\n",
    "### Description\n",
    "\n",
    "Alright, it's time to build your own ETL pipeline from scratch. In this exercise, you'll build three functions; ``extract()``, ``transform()``, and ``load()``. Then, you'll use these functions to run your pipeline.\n",
    "\n",
    "The ``pandas`` library has been imported as ``pd``. Enjoy!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* In the ``extract()`` function, use the appropriate pandas function to read a CSV into memory.\n",
    "* Update the ``transform()`` function to filter the ``data_frame`` to only include the columns ``industry_name`` and ``number_of_firms``.\n",
    "* In the ``load()`` function write the ``data_frame`` to a path stored using the parameter ``file_name``.\n",
    "* Pass the ``transformed_data`` DataFrame to the ``load()`` function, and run the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e69d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "  # Read a CSV with a path stored using file_name into memory\n",
    "  return pd.read_csv(file_name)\n",
    "\n",
    "def transform(data_frame):\n",
    "  # Filter the data_frame to only incude a subset of columns\n",
    "  return data_frame.loc[:, [\"industry_name\", \"number_of_firms\"]]\n",
    "\n",
    "def load(data_frame, file_name):\n",
    "  # Write the data_frame to a CSV\n",
    "  data_frame.to_csv(file_name)\n",
    "\n",
    "# Pass the transformed_data DataFrame to the load() function\n",
    "load(data_frame=transformed_data, file_name=\"number_of_firms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e723a04",
   "metadata": {},
   "source": [
    "Congrats, you just built a data pipeline from the ground-up. See you in the next chapter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
