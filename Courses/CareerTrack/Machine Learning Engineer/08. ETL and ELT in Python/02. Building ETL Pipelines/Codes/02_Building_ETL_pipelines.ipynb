{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353b33b4",
   "metadata": {},
   "source": [
    "# 2. Building ETL Pipelines\n",
    "\n",
    "Dive into leveraging pandas to extract, transform, and load data as you build your first data pipelines. Learn how to make your ETL logic reusable, and apply logging and exception handling to your pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b326e5f",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1daf1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Parquet Files\n",
    "import parquet as pq\n",
    "import fastparquet\n",
    "\n",
    "# SQL\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "\n",
    "# Pandas SQL\n",
    "import pandasql as ps\n",
    "\n",
    "# Logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf20ba",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddddcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_csv_path = \"../Datasets/sales_data.csv\"\n",
    "sales_data = pd.read_csv(sales_data_csv_path)\n",
    "\n",
    "clean_sales_data_csv_path = \"../Datasets/clean_sales_data.csv\"\n",
    "clean_sales_data = pd.read_csv(clean_sales_data_csv_path)\n",
    "\n",
    "sales_data_pq_path = \"../Datasets/sales_data.parquet\"\n",
    "sales_data.to_parquet(sales_data_pq_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ede388",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7ad58",
   "metadata": {},
   "source": [
    "## 1. Extracting data from parquet files\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the most common ways to ingest data from a source system is by reading data from a file, such as a CSV file. As data has gotten bigger, the need for better file formats has brought about new column-oriented file types, such as parquet files.\n",
    "\n",
    "In this exercise, you'll practice extracting data from a parquet file.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Read the parquet file at the path ``\"sales_data.parquet\"`` into a ``pandas`` DataFrame.\n",
    "* Check the data types of the DataFrame via ``print()``ing.\n",
    "* Output the shape of the DataFrame, as well as it's head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23bd59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order ID              int64\n",
      "Product              object\n",
      "Quantity Ordered      int64\n",
      "Price Each          float64\n",
      "Order Date           object\n",
      "Purchase Address     object\n",
      "dtype: object\n",
      "(282, 6)\n",
      "   Order ID                 Product  Quantity Ordered  Price Each  \\\n",
      "0    259358  34in Ultrawide Monitor                 1      379.99   \n",
      "1    259359  27in 4K Gaming Monitor                 1      389.99   \n",
      "2    259360  AAA Batteries (4-pack)                 2        2.99   \n",
      "3    259361        27in FHD Monitor                 1      149.99   \n",
      "4    259362        Wired Headphones                 1       11.99   \n",
      "\n",
      "       Order Date                           Purchase Address  \n",
      "0  10/28/19 10:56            609 Cherry St, Dallas, TX 75001  \n",
      "1  10/28/19 17:26          225 5th St, Los Angeles, CA 90001  \n",
      "2  10/24/19 17:20       967 12th St, New York City, NY 10001  \n",
      "3  10/14/19 22:26  628 Jefferson St, New York City, NY 10001  \n",
      "4   10/7/19 16:10         534 14th St, Los Angeles, CA 90001  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the sales data into a DataFrame\n",
    "sales_data = pd.read_parquet(sales_data_pq_path, engine=\"fastparquet\")\n",
    "\n",
    "# Check the data type of the columns of the DataFrames\n",
    "print(sales_data.dtypes)\n",
    "\n",
    "# Print the shape of the DataFrame, as well as the head\n",
    "print(sales_data.shape)\n",
    "print(sales_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e021f",
   "metadata": {},
   "source": [
    "Great job! Not only have you imported a parquet file, but you've investigated the DataFrame's metadata. This will help when tranforming data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1df21e",
   "metadata": {},
   "source": [
    "## 2. Pulling data from SQL databases\n",
    "\n",
    "### Description\n",
    "\n",
    "SQL databases are one of the most used data storage tools in the world. Many companies have teams of several individuals responsible for creating and maintaining these databases, which typically store data crucial for day-to-day operations. These SQL databases are commonly used as source systems for a wide range of data pipelines.\n",
    "\n",
    "For this exercise, ``pandas`` has been imported as ``pd``. Best of luck!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Update the connection URI to create a connection engine for the ``sales`` database, using ``sqlalchemy``.\n",
    "* Query all rows and columns of the ``sales`` table and output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9ae8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "# Create a connection to the sales database\n",
    "\n",
    "# db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/sales\") # Requires PostgreSQL Server running\n",
    "\n",
    "# Query all rows and columns of the sales table\n",
    "# raw_sales_data = pd.read_sql(\"SELECT * FROM sales\", db_engine)\n",
    "# print(raw_sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb218bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Order ID                   Product  Quantity Ordered  Price Each  \\\n",
      "0      259358    34in Ultrawide Monitor                 1      379.99   \n",
      "1      259359    27in 4K Gaming Monitor                 1      389.99   \n",
      "2      259360    AAA Batteries (4-pack)                 2        2.99   \n",
      "3      259361          27in FHD Monitor                 1      149.99   \n",
      "4      259362          Wired Headphones                 1       11.99   \n",
      "..        ...                       ...               ...         ...   \n",
      "277    259623  Apple Airpods Headphones                 1      150.00   \n",
      "278    259624    34in Ultrawide Monitor                 1      379.99   \n",
      "279    259625    AAA Batteries (4-pack)                 1        2.99   \n",
      "280    259626    AAA Batteries (4-pack)                 2        2.99   \n",
      "281    259627    AAA Batteries (4-pack)                 1        2.99   \n",
      "\n",
      "         Order Date                           Purchase Address  \n",
      "0    10/28/19 10:56            609 Cherry St, Dallas, TX 75001  \n",
      "1    10/28/19 17:26          225 5th St, Los Angeles, CA 90001  \n",
      "2    10/24/19 17:20       967 12th St, New York City, NY 10001  \n",
      "3    10/14/19 22:26  628 Jefferson St, New York City, NY 10001  \n",
      "4     10/7/19 16:10         534 14th St, Los Angeles, CA 90001  \n",
      "..              ...                                        ...  \n",
      "277  10/20/19 21:12      128 River St, San Francisco, CA 94016  \n",
      "278  10/30/19 12:45                60 9th St, Austin, TX 73301  \n",
      "279  10/30/19 18:37       946 13th St, San Francisco, CA 94016  \n",
      "280  10/22/19 15:15            419 South St, Seattle, WA 98101  \n",
      "281   10/9/19 20:05        30 Lake St, San Francisco, CA 94016  \n",
      "\n",
      "[282 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Alternative: PandaSQL\n",
    "\n",
    "query = \"SELECT * FROM sales\"\n",
    "raw_sales_data = ps.sqldf(query, {\"sales\": sales_data})\n",
    "print(raw_sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf69d3",
   "metadata": {},
   "source": [
    "Pretty cool, huh? Being able to leverage Python to pull directly from a SQL database will turbo-charge your data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1dd20",
   "metadata": {},
   "source": [
    "### 3. Building functions to extract data\n",
    "\n",
    "### Desccription\n",
    "\n",
    "It's important to modularize code when building a data pipeline. This helps to make pipelines more readable and reusable, and can help to expedite troubleshooting efforts. Creating and using functions for distinct operations in a pipeline can even help when getting started on a new project by providing a framework to begin development.\n",
    "\n",
    "``pandas`` has been imported as ``pd``, and ``sqlalchemy`` is ready to be used.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Complete the connection URI with port ``5432`` and database ``\"sales\"``.\n",
    "* Pass the URI to the appropriate ``sqlalchemy`` function to create a connection engine.\n",
    "* Use ``pandas`` to query the ``sales`` tables for all columns and records with ``\"quantity_ordered\"`` equal to 1.\n",
    "* Print the head of the DataFrame, and return the extracted data. Then, execute the ``extract()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53d3bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Order ID                 Product  Quantity Ordered  Price Each  \\\n",
      "0    259358  34in Ultrawide Monitor                 1      379.99   \n",
      "1    259359  27in 4K Gaming Monitor                 1      389.99   \n",
      "2    259360  AAA Batteries (4-pack)                 2        2.99   \n",
      "3    259361        27in FHD Monitor                 1      149.99   \n",
      "4    259362        Wired Headphones                 1       11.99   \n",
      "\n",
      "       Order Date                           Purchase Address  \n",
      "0  10/28/19 10:56            609 Cherry St, Dallas, TX 75001  \n",
      "1  10/28/19 17:26          225 5th St, Los Angeles, CA 90001  \n",
      "2  10/24/19 17:20       967 12th St, New York City, NY 10001  \n",
      "3  10/14/19 22:26  628 Jefferson St, New York City, NY 10001  \n",
      "4   10/7/19 16:10         534 14th St, Los Angeles, CA 90001  \n"
     ]
    }
   ],
   "source": [
    "def extract():\n",
    "  \t# Create a connection URI and connection engine\n",
    "    \n",
    "    # connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n",
    "    # db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "    # Query the DataFrame to return all records with quantity_ordered equal to 1\n",
    "    # raw_data = pd.read_sql(\"SELECT * FROM sales WHERE quantity_ordered = 1\", db_engine)\n",
    "\n",
    "    # Alternative: PandaSQL\n",
    "    query = \"SELECT * FROM sales\"\n",
    "    raw_data = ps.sqldf(query, {\"sales\": sales_data})\n",
    "\n",
    "    # Print the head of the DataFrame\n",
    "    print(raw_data.head())\n",
    "    \n",
    "    # Return the extracted DataFrame\n",
    "    return raw_data\n",
    "    \n",
    "# Call the extract() function\n",
    "raw_sales_data = extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471426de",
   "metadata": {},
   "source": [
    "Awesome work! Modularizing logic in a data pipeline helps to make your code reusable and easier to troubleshoot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd732e51",
   "metadata": {},
   "source": [
    "## 3. Filtering pandas DataFrames\n",
    "\n",
    "### Description\n",
    "\n",
    "Once data has been extracted from a source system, it's time to transform it! Often, source data may have more information than what is needed for downstream use cases. If this is the case, dimensionality should be reduced during the \"transform\" phase of the data pipeline.\n",
    "\n",
    "``pandas`` has been imported as ``pd``, and the ``extract()`` function is available to load a DataFrame from the path that is passed.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Use the ``extract()`` function to load the DataFrame stored in the ``\"sales_data.parquet\"`` path.\n",
    "* Update the ``transform()`` function to return all rows and columns with ``\"Quantity Ordered\"`` greater than 1.\n",
    "* Further filter the ``clean_data`` DataFrame to only include columns ``\"Order Date\"``, ``\"Quantity Ordered\"`` and ``\"Purchase Address\"``.\n",
    "* Return the filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccf8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f94ed0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/24/19 17:20</td>\n",
       "      <td>2</td>\n",
       "      <td>967 12th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10/31/19 19:06</td>\n",
       "      <td>2</td>\n",
       "      <td>263 Willow St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10/9/19 20:27</td>\n",
       "      <td>3</td>\n",
       "      <td>11 Lakeview St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10/1/19 21:32</td>\n",
       "      <td>2</td>\n",
       "      <td>110 Hill St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>10/16/19 19:13</td>\n",
       "      <td>2</td>\n",
       "      <td>280 8th St, Portland, OR 97035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10/18/19 8:54</td>\n",
       "      <td>2</td>\n",
       "      <td>791 Forest St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10/31/19 22:54</td>\n",
       "      <td>2</td>\n",
       "      <td>64 Lake St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>10/26/19 0:30</td>\n",
       "      <td>2</td>\n",
       "      <td>23 Walnut St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>10/18/19 21:47</td>\n",
       "      <td>2</td>\n",
       "      <td>145 River St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>10/6/19 12:22</td>\n",
       "      <td>3</td>\n",
       "      <td>512 1st St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>10/12/19 19:49</td>\n",
       "      <td>2</td>\n",
       "      <td>874 1st St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>10/23/19 20:00</td>\n",
       "      <td>2</td>\n",
       "      <td>670 West St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>10/16/19 14:48</td>\n",
       "      <td>2</td>\n",
       "      <td>68 Chestnut St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>10/14/19 11:54</td>\n",
       "      <td>4</td>\n",
       "      <td>817 Maple St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>10/3/19 13:05</td>\n",
       "      <td>2</td>\n",
       "      <td>237 Adams St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>10/19/19 17:38</td>\n",
       "      <td>3</td>\n",
       "      <td>730 Lakeview St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>10/2/19 16:01</td>\n",
       "      <td>2</td>\n",
       "      <td>387 12th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10/19/19 7:27</td>\n",
       "      <td>4</td>\n",
       "      <td>545 7th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>10/22/19 0:11</td>\n",
       "      <td>2</td>\n",
       "      <td>227 Lake St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>10/22/19 18:05</td>\n",
       "      <td>2</td>\n",
       "      <td>113 Spruce St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>10/24/19 16:14</td>\n",
       "      <td>2</td>\n",
       "      <td>359 6th St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>10/21/19 9:25</td>\n",
       "      <td>2</td>\n",
       "      <td>358 8th St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>10/31/19 2:08</td>\n",
       "      <td>2</td>\n",
       "      <td>312 Highland St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>10/21/19 10:38</td>\n",
       "      <td>2</td>\n",
       "      <td>50 7th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>10/26/19 10:53</td>\n",
       "      <td>2</td>\n",
       "      <td>668 Church St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>10/1/19 5:32</td>\n",
       "      <td>3</td>\n",
       "      <td>434 Center St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>10/18/19 10:06</td>\n",
       "      <td>5</td>\n",
       "      <td>165 Lincoln St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>10/2/19 3:48</td>\n",
       "      <td>2</td>\n",
       "      <td>435 Cedar St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>10/9/19 3:13</td>\n",
       "      <td>2</td>\n",
       "      <td>594 Lincoln St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>10/25/19 15:00</td>\n",
       "      <td>4</td>\n",
       "      <td>554 North St, Portland, OR 97035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>10/2/19 22:45</td>\n",
       "      <td>3</td>\n",
       "      <td>282 North St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>10/6/19 22:34</td>\n",
       "      <td>3</td>\n",
       "      <td>809 Sunset St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>10/22/19 15:15</td>\n",
       "      <td>2</td>\n",
       "      <td>419 South St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Order Date  Quantity Ordered  \\\n",
       "2    10/24/19 17:20                 2   \n",
       "41   10/31/19 19:06                 2   \n",
       "44    10/9/19 20:27                 3   \n",
       "52    10/1/19 21:32                 2   \n",
       "57   10/16/19 19:13                 2   \n",
       "63    10/18/19 8:54                 2   \n",
       "68   10/31/19 22:54                 2   \n",
       "80    10/26/19 0:30                 2   \n",
       "93   10/18/19 21:47                 2   \n",
       "113   10/6/19 12:22                 3   \n",
       "125  10/12/19 19:49                 2   \n",
       "146  10/23/19 20:00                 2   \n",
       "156  10/16/19 14:48                 2   \n",
       "157  10/14/19 11:54                 4   \n",
       "158   10/3/19 13:05                 2   \n",
       "182  10/19/19 17:38                 3   \n",
       "186   10/2/19 16:01                 2   \n",
       "190   10/19/19 7:27                 4   \n",
       "192   10/22/19 0:11                 2   \n",
       "196  10/22/19 18:05                 2   \n",
       "200  10/24/19 16:14                 2   \n",
       "216   10/21/19 9:25                 2   \n",
       "219   10/31/19 2:08                 2   \n",
       "220  10/21/19 10:38                 2   \n",
       "223  10/26/19 10:53                 2   \n",
       "226    10/1/19 5:32                 3   \n",
       "246  10/18/19 10:06                 5   \n",
       "251    10/2/19 3:48                 2   \n",
       "254    10/9/19 3:13                 2   \n",
       "264  10/25/19 15:00                 4   \n",
       "268   10/2/19 22:45                 3   \n",
       "269   10/6/19 22:34                 3   \n",
       "280  10/22/19 15:15                 2   \n",
       "\n",
       "                             Purchase Address  \n",
       "2        967 12th St, New York City, NY 10001  \n",
       "41     263 Willow St, San Francisco, CA 94016  \n",
       "44          11 Lakeview St, Seattle, WA 98101  \n",
       "52       110 Hill St, San Francisco, CA 94016  \n",
       "57             280 8th St, Portland, OR 97035  \n",
       "63       791 Forest St, Los Angeles, CA 90001  \n",
       "68              64 Lake St, Atlanta, GA 30301  \n",
       "80      23 Walnut St, San Francisco, CA 94016  \n",
       "93        145 River St, Los Angeles, CA 90001  \n",
       "113              512 1st St, Austin, TX 73301  \n",
       "125       874 1st St, New York City, NY 10001  \n",
       "146        670 West St, Los Angeles, CA 90001  \n",
       "156   68 Chestnut St, New York City, NY 10001  \n",
       "157     817 Maple St, San Francisco, CA 94016  \n",
       "158       237 Adams St, Los Angeles, CA 90001  \n",
       "182  730 Lakeview St, New York City, NY 10001  \n",
       "186      387 12th St, San Francisco, CA 94016  \n",
       "190       545 7th St, New York City, NY 10001  \n",
       "192             227 Lake St, Boston, MA 02215  \n",
       "196          113 Spruce St, Seattle, WA 98101  \n",
       "200              359 6th St, Dallas, TX 75001  \n",
       "216             358 8th St, Seattle, WA 98101  \n",
       "219  312 Highland St, New York City, NY 10001  \n",
       "220        50 7th St, New York City, NY 10001  \n",
       "223    668 Church St, San Francisco, CA 94016  \n",
       "226          434 Center St, Seattle, WA 98101  \n",
       "246     165 Lincoln St, Los Angeles, CA 90001  \n",
       "251           435 Cedar St, Seattle, WA 98101  \n",
       "254     594 Lincoln St, Los Angeles, CA 90001  \n",
       "264          554 North St, Portland, OR 97035  \n",
       "268     282 North St, New York City, NY 10001  \n",
       "269      809 Sunset St, Los Angeles, CA 90001  \n",
       "280           419 South St, Seattle, WA 98101  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data from the sales_data.parquet path\n",
    "raw_sales_data = extract(sales_data_pq_path)\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Only keep rows with `Quantity Ordered` greater than 1\n",
    "    clean_data = raw_data.loc[raw_data[\"Quantity Ordered\"] > 1, :]\n",
    "\t\n",
    "    # Only keep columns \"Order Date\", \"Quantity Ordered\", and \"Purchase Address\"\n",
    "    clean_data = clean_data.loc[:, [\"Order Date\", \"Quantity Ordered\", \"Purchase Address\"]]\n",
    "\t\n",
    "    # Return the filtered DataFrame\n",
    "    return clean_data\n",
    "    \n",
    "transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4e97b",
   "metadata": {},
   "source": [
    "Perfect transformation! Removing rows from a DataFrame is one of the most common operations you'll perform when working with DataFrames. Keep up the great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcad679",
   "metadata": {},
   "source": [
    "## 4. Transforming sales data with pandas\n",
    "\n",
    "### Description\n",
    "\n",
    "Before insights can be extracted from a dataset, column types may need to be altered to properly leverage the data. This is especially common with temporal data types, which can be stored in several different ways.\n",
    "\n",
    "For this example, ``pandas`` has been import as ``pd`` and is ready for you to use.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Update the ``transform()`` function to convert data in the ``\"Order Date\"`` column to type datetime.\n",
    "* Filter the DataFrame to only contain rows with ``\"Price Each\"`` less than ten dollars.\n",
    "* Print the data types of each column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02477d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    # Ingest the data to a DataFrame\n",
    "    \n",
    "    raw_data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3330968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order ID                     int64\n",
      "Product                     object\n",
      "Quantity Ordered             int64\n",
      "Price Each                 float64\n",
      "Order Date          datetime64[ns]\n",
      "Purchase Address            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "raw_sales_data = extract(sales_data_csv_path)\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Convert the \"Order Date\" column to type datetime\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    \n",
    "    # Only keep items under ten dollars\n",
    "    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n",
    "    return clean_data\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Check the data types of each column\n",
    "print(clean_sales_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00219904",
   "metadata": {},
   "source": [
    "Very well done! Datestamps and timestamps are everywhere in data, and they can get messy. Being able to transform them is an invaluable skill!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23d447e",
   "metadata": {},
   "source": [
    "## 5. Validating data transformations\n",
    "\n",
    "### Description\n",
    "\n",
    "Great work so far! Manually spot-checking transformations is a great first step to ensuring that you're maintaining data quality throughout a pipeline. ``pandas`` offers several built-in functions to help you with just that!\n",
    "\n",
    "To help get you started with this exercise, ``pandas`` has been imported as ``pd``.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Update the ``extract()`` function to read the parquet file stored in the ``file_path`` parameter into a DataFrame.\n",
    "* Update the ``transform()`` function to return the ``\"Order ID\"``, ``\"Price Each\"`` and ``\"Quantity Ordered\"`` columns for all items with a ``\"Quantity Ordered\"`` equal to 1.\n",
    "* Call the ``transform()`` function on the ``raw_sales_data`` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2c93499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "  \t# Ingest the data to a DataFrame\n",
    "    raw_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return raw_data\n",
    "  \n",
    "raw_sales_data = extract(sales_data_pq_path)\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Filter rows and columns\n",
    "    clean_data = raw_data.loc[raw_data[\"Quantity Ordered\"] == 1, [\"Order ID\", \"Price Each\", \"Quantity Ordered\"]]\n",
    "    return clean_data\n",
    "\n",
    "# Transform the raw_sales_data\n",
    "clean_sales_data = transform(raw_sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eae6d7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1700.0, 1700.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(clean_sales_data[\"Price Each\"], reverse=True)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f9c1c",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What is the value of the price ``\"Price Each\"`` column of the two most expensive items in the transformed DataFrame? The ``clean_sales_data`` DataFrame has been loaded for you, and you can use the console to explore it further.\n",
    "\n",
    "### Answer\n",
    "\n",
    "`1700` as shown above. \n",
    "\n",
    "Congrats! You've not only built the 'extract' and 'load' components of a pipeline, but you've taken steps to validate the logic that you've implemented. Great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eee2aa",
   "metadata": {},
   "source": [
    "## 6. Loading sales data to a CSV file\n",
    "\n",
    "### Description\n",
    "\n",
    "Loading data is an essential component of any data pipeline. It ensures that any data consumers and processes have reliable access to data that you've extracted and transformed earlier in a pipeline. In this exercise, you'll practice loading transformed sales data to a CSV file using ``pandas``, which has been imported as ``pd``. In addition to this, the raw data has been extracted and is available in the DataFrame ``raw_sales_data``.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Filter the ``raw_sales_data`` DataFrame to only keep all items with a price less than 25 dollars.\n",
    "* Update the ``load()`` function to write the transformed sales data to a file named ``\"transformed_sales_data.csv\"``, making sure not include the ``index`` column.\n",
    "* Call the ``load()`` function on the cleaned Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "979e2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Find the items prices less than 25 dollars\n",
    "\treturn raw_data.loc[raw_data[\"Price Each\"] < 25, [\"Order ID\", \"Product\", \"Price Each\", \"Order Date\"]]\n",
    "\n",
    "def load(clean_data):\n",
    "\t# Write the data to a CSV file without the index column\n",
    "\tclean_data.to_csv(\"transformed_sales_data.csv\", index=False)\n",
    "\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Call the load function on the cleaned DataFrame\n",
    "load(clean_sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec608d2",
   "metadata": {},
   "source": [
    "Fantastic work! You've loaded data that had been extracted and transformed to a CSV file, where it can be used by data consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1a1aa",
   "metadata": {},
   "source": [
    "## 7. Customizing a CSV file\n",
    "\n",
    "### Description\n",
    "\n",
    "Sometimes, data needs to be stored in a CSV file in a customized manner. This may include using different header values, including or excluding the index column of a DataFrame, or altering the character used to separate columns. In this example, you'll get to practice this, as well as ensuring that the file is stored in the desired file path.\n",
    "\n",
    "The ``pandas`` library has been imported as ``pd``, and the data has already been transformed to include only rows with a \"Quantity Ordered\" greater than one. The cleaned DataFrame is stored in a variable named ``clean_sales_data``.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Import the ``os`` library.\n",
    "* Write the cleaned DataFrame to a CSV stored at ``path_to_write``, without a header.\n",
    "* Ensure the file was written to the desired path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8abbe5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import the os library\n",
    "import os\n",
    "\n",
    "# Load the data to a csv file with the index, no header and pipe separated\n",
    "def load(clean_data, path_to_write):\n",
    "\tclean_data.to_csv(path_to_write, header=False, sep=\"|\")\n",
    "\n",
    "load(clean_sales_data, \"../Datasets/clean_sales_data.csv\")\n",
    "\n",
    "# Check that the file is present.\n",
    "file_exists = os.path.exists(\"../Datasets/clean_sales_data.csv\")\n",
    "print(file_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b32b2",
   "metadata": {},
   "source": [
    "Perfect! You've successfully written a DataFrame to a CSV file with custom options, and validated that the file was written to the desired path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72eba7b",
   "metadata": {},
   "source": [
    "## 8. Persisting data to files\n",
    "\n",
    "### Description\n",
    "\n",
    "Loading data to a final destination is one of the most important steps of a data pipeline. In this exercise, you'll use the ``transform()`` function shown below to transform product ``sales`` data before loading it to a ``.csv`` file. This will give downstream data consumers a better view into total sales across a range of products.\n",
    "\n",
    "For this exercise, the ``sales`` data has been loaded and transformed, and is stored in the ``clean_sales_data`` DataFrame. The ``pandas`` package has been imported as ``pd``, and the ``os`` library is also ready to use!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Update the ``load()`` function to write data to the provided path, without headers or an index column.\n",
    "* Check to make sure the file was loaded to the desired file path.\n",
    "* Call the function to load the transformed data to persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a86e62a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, file_path):\n",
    "    # Write the data to a file\n",
    "    clean_data.to_csv(file_path, header=False, index=False)\n",
    "\n",
    "    # Check to make sure the file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    if not file_exists:\n",
    "        raise Exception(f\"File does NOT exists at path {file_path}\")\n",
    "\n",
    "# Load the transformed data to the provided file path\n",
    "load(clean_sales_data, \"../Datasets/transformed_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabed241",
   "metadata": {},
   "source": [
    "Great job! You've successfully loaded a DataFrame to a .csv file and ensured the file existed after writing it. This is an essential component of building reliable data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b81bc",
   "metadata": {},
   "source": [
    "## 9. Logging within a data pipeline\n",
    "\n",
    "### Description\n",
    "\n",
    "In this exercise, we'll take a look back at the function you wrote in a previous video and practice adding logging to the function. This will help when troubleshooting errors or making changes to the logic!\n",
    "\n",
    "``pandas`` has been imported as ``pd``. In addition to this, the ``logging`` module has been imported, and the default log-level has been set to ``\"debug\"``.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create an info-level log after the transformation, passing the string: ``\"Transformed 'Order Date' column to type 'datetime'.\"``\n",
    "* Log the ``.shape`` of the DataFrame at the debug-level before and after filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "842078b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n",
    "    \n",
    "    # Create an info log regarding transformation\n",
    "    logging.info(\"Transformed 'Order Date' column to type 'datetime'.\")\n",
    "    \n",
    "    # Create debug-level logs for the DataFrame before and after filtering\n",
    "    logging.debug(f\"Shape of the DataFrame before filtering: {raw_data.shape}\")\n",
    "    logging.debug(f\"Shape of the DataFrame after filtering: {clean_data.shape}\")\n",
    "    \n",
    "    return clean_data\n",
    "  \n",
    "clean_sales_data = transform(raw_sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fd7b2",
   "metadata": {},
   "source": [
    "Nicely done! Creating logs to provide meaningful output can save a lot of time when a pipeline fails or when a change needs to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfa462",
   "metadata": {},
   "source": [
    "## 10. Handling exceptions when loading data\n",
    "\n",
    "### Description\n",
    "\n",
    "Sometimes, your data pipelines might throw an exception. These exceptions are a form of alerting, and they let a Data Engineer know when something unexpected happened. It's important to properly handle these exceptions. In this exercise, we'll practice just that!\n",
    "\n",
    "To help get you started, ``pandas`` has been imported as ``pd``, along with the ``logging`` module has been imported. The default log-level has been set to ``\"debug\"``.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Update the pipeline to include a ``try`` block, and attempt to read the data from the path ``\"sales_data.parquet\"``.\n",
    "* Catch a ``FileNotFoundError`` if the file is not able to be read into a ``pandas`` DataFrame.\n",
    "* Create an error-level log to document the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75ec1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "# Update the pipeline to include a try block\n",
    "try:\n",
    "\t# Attempt to read in the file\n",
    "    raw_sales_data = extract(sales_data_pq_path)\n",
    "\t\n",
    "# Catch the FileNotFoundError\n",
    "except FileNotFoundError as file_not_found:\n",
    "\t# Write an error-level log\n",
    "\tlogging.error(file_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3b0cc",
   "metadata": {},
   "source": [
    "Wonderful! Incorporating try-except logic in your pipelines is the foundation for more advanced monitoring and alerting solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd1d0c",
   "metadata": {},
   "source": [
    "## 11. Monitoring and alerting within a data pipeline\n",
    "\n",
    "### Description\n",
    "\n",
    "It's time to put it all together! You might have guessed it, but using handling errors using try-except and logging go hand-in-hand. These two practices are essential for a pipeline to be resilient and transparent, and are the building blocks for more advanced monitoring and alerting solutions.\n",
    "\n",
    "``pandas`` has been imported as ``pd``, and the ``logging`` module has been loaded and configured for you. The ``raw_sales_data`` DataFrame has been extracted, and is ready to be transformed.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create an info-level logging message to document success, and a warning-level logging message if the transformation fails.\n",
    "* Update the ``try-except`` clause to catch a ``KeyError``, and alias as ``ke``.\n",
    "* Change the warning-level log to include the error being thrown.\n",
    "* If a key error is thrown, create a column ``\"Total Price\"`` by multiplying the ``\"Price Each\"`` and ``\"Quantity Ordered\"`` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df0c2265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259358</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>10/28/19 10:56</td>\n",
       "      <td>609 Cherry St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259359</td>\n",
       "      <td>27in 4K Gaming Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>389.99</td>\n",
       "      <td>10/28/19 17:26</td>\n",
       "      <td>225 5th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259360</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>2</td>\n",
       "      <td>2.99</td>\n",
       "      <td>10/24/19 17:20</td>\n",
       "      <td>967 12th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259361</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>149.99</td>\n",
       "      <td>10/14/19 22:26</td>\n",
       "      <td>628 Jefferson St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259362</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>10/7/19 16:10</td>\n",
       "      <td>534 14th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order ID                 Product  Quantity Ordered  Price Each  \\\n",
       "0    259358  34in Ultrawide Monitor                 1      379.99   \n",
       "1    259359  27in 4K Gaming Monitor                 1      389.99   \n",
       "2    259360  AAA Batteries (4-pack)                 2        2.99   \n",
       "3    259361        27in FHD Monitor                 1      149.99   \n",
       "4    259362        Wired Headphones                 1       11.99   \n",
       "\n",
       "       Order Date                           Purchase Address  \n",
       "0  10/28/19 10:56            609 Cherry St, Dallas, TX 75001  \n",
       "1  10/28/19 17:26          225 5th St, Los Angeles, CA 90001  \n",
       "2  10/24/19 17:20       967 12th St, New York City, NY 10001  \n",
       "3  10/14/19 22:26  628 Jefferson St, New York City, NY 10001  \n",
       "4   10/7/19 16:10         534 14th St, Los Angeles, CA 90001  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data_csv_path = \"../Datasets/sales_data_backup.csv\"\n",
    "raw_sales_data = pd.read_csv(sales_data_csv_path)\n",
    "raw_sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c006393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'Total Price': Cannot filter DataFrame by 'Total Price'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'except Exception:\\n\\t# Log a warning-level message\\n\\tlogging.warning(\"Cannot filter DataFrame by \\'Total Price\\'\")'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(raw_data):\n",
    "\treturn raw_data.loc[raw_data[\"Total Price\"] > 1000, :]\n",
    "\n",
    "try:\n",
    "\t# Attempt to transform DataFrame, log an info-level message\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\tlogging.info(\"Successfully filtered DataFrame by 'Total Price'\")\n",
    "# Update the exception to be a KeyError, alias as ke\n",
    "except KeyError as ke:\n",
    "\tlogging.warning(f\"{ke}: Cannot filter DataFrame by 'Total Price'\")\n",
    "\t\n",
    "\t# Create the \"Total Price\" column, transform the updated DataFrame\n",
    "\traw_sales_data[\"Total Price\"] = raw_sales_data[\"Price Each\"] * raw_sales_data[\"Quantity Ordered\"]\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "'''except Exception:\n",
    "\t# Log a warning-level message\n",
    "\tlogging.warning(\"Cannot filter DataFrame by 'Total Price'\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9e3d0",
   "metadata": {},
   "source": [
    "Great job! Adding monitoring and alerting to your data pipelines will help to build trust in your solutions. Keep up the great work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
