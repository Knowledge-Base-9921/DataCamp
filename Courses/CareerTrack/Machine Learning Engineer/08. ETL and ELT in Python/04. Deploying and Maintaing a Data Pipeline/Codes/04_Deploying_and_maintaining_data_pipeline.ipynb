{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1c6874",
   "metadata": {},
   "source": [
    "# 4. Deploying and Maintaining a Data Pipeline\n",
    "\n",
    "In this final chapter, you’ll create frameworks to validate and test data pipelines before shipping them into production. After you’ve tested your pipeline, you’ll explore techniques to run your data pipeline end-to-end, all while allowing for visibility into pipeline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ce181",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1436ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Json\n",
    "import json\n",
    "\n",
    "# Pytest\n",
    "import pytest\n",
    "\n",
    "# Parquet Files\n",
    "import parquet as pq\n",
    "import fastparquet\n",
    "\n",
    "# SQL\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "\n",
    "# Pandas SQL\n",
    "import pandasql as ps\n",
    "\n",
    "# Logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d5fd3",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0946fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data_csv_path = \"../Datasets/raw_tax_data.csv\"\n",
    "raw_tax_data = pd.read_csv(raw_tax_data_csv_path)\n",
    "\n",
    "clean_tax_data_csv_path = \"../Datasets/clean_tax_data.csv\"\n",
    "clean_tax_data_csv = pd.read_csv(clean_tax_data_csv_path)\n",
    "\n",
    "clean_tax_data_pq_path = \"../Datasets/clean_tax_data.parquet\"\n",
    "clean_tax_data_csv.to_parquet(clean_tax_data_pq_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9c9d2",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854823c",
   "metadata": {},
   "source": [
    "## 1. Quiz: Testing data pipelines\n",
    "\n",
    "### Description\n",
    "\n",
    "Validating a data pipeline is one of the most important measures that a Data Engineer can take to ensure that a pipeline will perform as expected when deployed to production.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Select all the benefits of validating a pipeline during and after development.\n",
    "\n",
    "### Answers\n",
    "\n",
    "* Improves reliability and trust in pipelined data\n",
    "* Validate that data is extracted, transformed, and loaded as expected\n",
    "* Helps to identify and avoid data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae26494",
   "metadata": {},
   "source": [
    "Awesome job! Understanding why it's important to validate a pipeline before deployment will help to motivate and shape testing efforts both during and after pipeline development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87786d4c",
   "metadata": {},
   "source": [
    "## 2. Validating a data pipeline at \"checkpoints\"\n",
    "\n",
    "### Description\n",
    "\n",
    "In this exercise, you'll be working with a data pipeline that extracts tax data from a CSV file, creates a new column, filters out rows based on average taxable income, and persists the data to a parquet file.\n",
    "\n",
    "``pandas`` has been loaded as ``pd``, and the ``extract()``, ``transform()``, and ``load()`` functions have already been defined. You'll use these functions to validate the data pipeline at various checkpoints throughout its execution.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Print the shape of the ``raw_tax_data`` and ``clean_tax_data`` DataFrames and observe the difference in dimensions.\n",
    "* Read the DataFrame from the path ``\"clean_tax_data.parquet\"`` into a DataFrame called ``to_validate``, observe the ``.head()`` of each.\n",
    "* Check that the ``to_validate`` and ``clean_tax_data`` DataFrames are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305487e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Find average taxable income for each business type.\n",
    "    raw_data[\"average_taxable_income\"] = raw_data[\"total_taxable_income\"] / raw_data[\"number_of_firms\"]\n",
    "    \n",
    "    # Only keep records with average_taxable_income > 100.\n",
    "    clean_data = raw_data.loc[raw_data[\"average_taxable_income\"] > 100, :]\n",
    "    \n",
    "    # Set the index to the industry_name.\n",
    "    clean_data.set_index(\"industry_name\", inplace=True)\n",
    "    \n",
    "    # Return the clean DataFrame.\n",
    "    return clean_data\n",
    "\n",
    "def load(clean_data, clean_data_path):\n",
    "    clean_data.to_parquet(clean_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69431324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw_tax_data: (96, 6)\n",
      "Shape of clean_tax_data: (82, 5)\n",
      "                   number_of_firms  total_taxable_income  total_taxes_paid  \\\n",
      "industry_name                                                                \n",
      "Aerospace/Defense               77             30920.169          5106.376   \n",
      "Apparel                         39              5422.690          1112.113   \n",
      "Auto & Truck                    31             33358.200          3529.000   \n",
      "\n",
      "                   total_cash_taxes_paid  average_taxable_income  \n",
      "industry_name                                                     \n",
      "Aerospace/Defense               7441.776              401.560636  \n",
      "Apparel                         1479.292              139.043333  \n",
      "Auto & Truck                    2446.896             1076.070968  \n",
      "                   number_of_firms  total_taxable_income  total_taxes_paid  \\\n",
      "industry_name                                                                \n",
      "Aerospace/Defense               77             30920.169          5106.376   \n",
      "Apparel                         39              5422.690          1112.113   \n",
      "Auto & Truck                    31             33358.200          3529.000   \n",
      "\n",
      "                   total_cash_taxes_paid  average_taxable_income  \n",
      "industry_name                                                     \n",
      "Aerospace/Defense               7441.776              401.560636  \n",
      "Apparel                         1479.292              139.043333  \n",
      "Auto & Truck                    2446.896             1076.070968  \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Extract and transform tax_data\n",
    "raw_tax_data = extract(raw_tax_data_csv_path)\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, clean_tax_data_pq_path)\n",
    "\n",
    "# Check the shape of the raw_tax_data DataFrame, compare to the clean_tax_data DataFrame\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "# Read in the loaded data, observe the head of each\n",
    "to_validate = pd.read_parquet(clean_tax_data_pq_path)\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))\n",
    "\n",
    "# Check that the DataFrames are equal\n",
    "print(to_validate.equals(clean_tax_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b14c4",
   "metadata": {},
   "source": [
    "Fantastic validation! Validating data as it flows through a pipeline ensures that the pipeline performs as it should, and can help catch bugs of faulty logic before the solution is deployed into production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60da10b",
   "metadata": {},
   "source": [
    "## 3. Testing a data pipeline end-to-end\n",
    "\n",
    "### Description\n",
    "\n",
    "In this exercise, you'll be working with the same data pipeline as before, which extracts, transforms, and loads tax data. You'll practice testing this pipeline end-to-end to ensure the solution can be run multiple times, without duplicating the transformed data in the parquet file.\n",
    "\n",
    "``pandas`` has been loaded as ``pd``, and the ``extract()``, ``transform()``, and ``load()`` functions have already been defined.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Run the ETL pipeline three times, using a ``for``-loop.\n",
    "* Print the shape of the ``clean_tax_data`` in each iteration of the pipeline run.\n",
    "* Read the DataFrame stored in the ``\"clean_tax_data.parquet\"`` file into the ``to_validate`` variable.\n",
    "* Output the shape of the ``to_validate`` DataFrame, comparing it to the shape of ``clean_tax_rate`` to ensure data wasn't duplicated upon each pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58a7c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt: 0\n",
      "Shape of clean_tax_data: (82, 5)\n",
      "Attempt: 1\n",
      "Shape of clean_tax_data: (82, 5)\n",
      "Attempt: 2\n",
      "Shape of clean_tax_data: (82, 5)\n",
      "Final shape of cleaned data: (82, 5)\n"
     ]
    }
   ],
   "source": [
    "# Trigger the data pipeline to run three times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(raw_tax_data_csv_path)\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\t\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "    \n",
    "# Read in the loaded data, check the shape\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {to_validate.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac44918",
   "metadata": {},
   "source": [
    "Great work! By testing this pipeline end-to-end, you've validated that the pipeline can be run multiple times, with data being made available to downstream consumers without duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbbf03",
   "metadata": {},
   "source": [
    "## 4. Validating a data pipeline with assert\n",
    "\n",
    "### Description\n",
    "\n",
    "To build unit tests for data pipelines, it's important to get familiar with the ``assert`` keyword, and the ``isinstance()`` function. In this exercise, you'll practice using these two tools to validate components of a data pipeline.\n",
    "\n",
    "The functions ``extract()`` and ``transform()`` have been made available for you, along with ``pandas``, which has been imported as ``pd``. Both ``extract()`` and ``transform()`` return a DataFrame. Good luck!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Assert that the ``clean_tax_data`` DataFrame has five columns.\n",
    "* Validate that the object stored in the ``clean_tax_data`` variable is an instance of a ``pd.DataFrame``.\n",
    "* Assert that the value stored in the ``clean_tax_data`` variable is an instance of ``pd.DataFrame``.\n",
    "* Try asserting that ``clean_tax_data`` takes the type ``str``, and observe the exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55645f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def transform(raw_data):\n",
    "    raw_data[\"average_taxable_income\"] = raw_data[\"total_taxable_income\"] / raw_data[\"number_of_firms\"]\n",
    "    clean_data = raw_data.loc[raw_data[\"average_taxable_income\"] > 100, :]\n",
    "    clean_data.set_index(\"industry_name\", inplace=True)\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480616e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_tax_data = extract(raw_tax_data_csv_path)\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Validate the number of columns in the DataFrame\n",
    "assert len(clean_tax_data.columns) == 5\n",
    "\n",
    "# Determine if the clean_tax_data DataFrames take type pd.DataFrame\n",
    "isinstance(clean_tax_data, pd.DataFrame)\n",
    "\n",
    "# Assert that clean_tax_data is an instance of a pd.DataFrame\n",
    "assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "\n",
    "# Assert that clean_tax_data takes is an instance of a string\n",
    "try:\n",
    "\tassert isinstance(clean_tax_data, str)\n",
    "except Exception as e:\n",
    "\tprint(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35185992",
   "metadata": {},
   "source": [
    "Super work! You leveraged ``isinstance()`` to validate data types, and ``assert`` to ensure that boolean expression return ``True``. Getting comfortable with these tools will help when writing unit tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c0808",
   "metadata": {},
   "source": [
    "## 5. Writing unit tests with pytest\n",
    "\n",
    "### Description\n",
    "\n",
    "In this exercise, you'll practice writing a unit test to validate a data pipeline. You'll use assert and other tools to build the tests, and determine if the data pipeline performs as it should.\n",
    "\n",
    "The functions ``extract()`` and ``transform()`` have been made available for you, along with ``pandas``, which has been imported as ``pd``. You'll be testing the ``transform()`` function, which is shown below.\n",
    "\n",
    "```py\n",
    "def transform(raw_data):\n",
    "    raw_data[\"average_taxable_income\"] = raw_data[\"total_taxable_income\"] / raw_data[\"number_of_firms\"]\n",
    "    clean_data = raw_data.loc[raw_data[\"average_taxable_income\"] > 100, :]\n",
    "    clean_data.set_index(\"industry_name\", inplace=True)\n",
    "    return clean_data\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Import the ``pytest`` library.\n",
    "* Assert that the value stored in the ``clean_tax_data`` variables is an instance of a ``pd.DataFrame``.\n",
    "* Validate that the number of columns in the ``clean_tax_data`` DataFrame is greater than the columns stored in the ``raw_tax_data`` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8fa02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "    raw_data[\"average_taxable_income\"] = raw_data[\"total_taxable_income\"] / raw_data[\"number_of_firms\"]\n",
    "    clean_data = raw_data.loc[raw_data[\"average_taxable_income\"] > 100, :]\n",
    "    clean_data.set_index(\"industry_name\", inplace=True)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec0492c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def test_transformed_data():\n",
    "    raw_tax_data = extract(raw_tax_data_csv_path)\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "    \n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "    \n",
    "    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame\n",
    "    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341574c",
   "metadata": {},
   "source": [
    "There you go! Building unit tests with pytest is as easy as creating and evaluating basic boolean statements with the help of the assert keyword. Keep up the great work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ccc99",
   "metadata": {},
   "source": [
    "## 6. Creating fixtures with pytest\n",
    "\n",
    "### Description\n",
    "\n",
    "When building unit tests, you'll sometimes have to do a bit of setup before testing can begin. Doing this setup within a unit test can make the tests more difficult to read, and may have to be repeated several times. Luckily, ``pytest`` offers a way to solve these problems, with fixtures.\n",
    "\n",
    "For this exercise, ``pandas`` has been imported as ``pd``, and the ``extract()`` function shown below is available for use!\n",
    "\n",
    "```py\n",
    "def extract(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Import the ``pytest`` library.\n",
    "* Create a ``pytest`` fixture called ``raw_tax_data``.\n",
    "* Return the ``raw_data`` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91222920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6dbdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytest\n",
    "import pytest\n",
    "\n",
    "# Create a pytest fixture\n",
    "@pytest.fixture()\n",
    "def raw_tax_data():\n",
    "\traw_data = extract(raw_tax_data_csv_path)\n",
    "   \n",
    "    # Return the raw DataFrame\n",
    "\treturn raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b09be",
   "metadata": {},
   "source": [
    "Fantastic fixtures! Creating pytest fixtures helps to keep unit test more concise, and helps to separate test set up from actual testing logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eaed60",
   "metadata": {},
   "source": [
    "## 7. Unit testing a data pipeline with fixtures\n",
    "\n",
    "### Description\n",
    "\n",
    "You've learned in the last video that unit testing can help to instill more trust in your data pipeline, and can even help to catch bugs throughout development. In this exercise, you'll practice writing both fixtures and unit tests, using the ``pytest`` library and ``assert``.\n",
    "\n",
    "The ``transform`` function that you'll be building unit tests around is shown below for reference. ``pandas`` has been imported as ``pd``, and the ``pytest()`` library is loaded and ready for use.\n",
    "\n",
    "```py\n",
    "def transform(raw_data):\n",
    "    raw_data[\"tax_rate\"] = raw_data[\"total_taxes_paid\"] / raw_data[\"total_taxable_income\"]\n",
    "    raw_data.set_index(\"industry_name\", inplace=True)\n",
    "    return raw_data\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create a ``pytest`` fixture called ``clean_tax_data``.\n",
    "* Apply the ``transform()`` function to the ``raw_data`` dataset, and save the result in the ``clean_data`` variable and return it.\n",
    "* Create a unit test using the fixture defined from the last step.\n",
    "* Complete the statement that ensures all values in the ``\"tax_rate\"`` column lie within the values 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f67ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "    raw_data[\"tax_rate\"] = raw_data[\"total_taxes_paid\"] / raw_data[\"total_taxable_income\"]\n",
    "    raw_data.set_index(\"industry_name\", inplace=True)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbb73eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pytest fixture\n",
    "@pytest.fixture()\n",
    "def clean_tax_data():\n",
    "    raw_data = pd.read_csv(raw_tax_data_csv_path)\n",
    "    \n",
    "    # Transform the raw_data, store in clean_data DataFrame, and return the variable\n",
    "    clean_data = transform(raw_data)\n",
    "    return clean_data\n",
    "\n",
    "\n",
    "# Pass the fixture to the function\n",
    "def test_tax_rate(clean_tax_data):\n",
    "    # Assert values are within the expected range\n",
    "    assert clean_tax_data[\"tax_rate\"].max() <= 1 and clean_tax_data[\"tax_rate\"].min() >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cba8d",
   "metadata": {},
   "source": [
    "Awesome work! Using fixtures and unit tests together help to make tests both easy to read, and easy to write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3855a7d",
   "metadata": {},
   "source": [
    "## 8. Quiz: Orchestration and ETL tools\n",
    "\n",
    "### Description\n",
    "\n",
    "When deploying data pipelines to production, Data Engineers need to make sure that their pipelines can run consistently on a schedule, have access to a flexible quantity of resources, and alert on failure. To do this, Data Engineers will often look outside of a Python script to an orchestration and ETL tool.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "What is the most popular orchestration tool for building, deploying, and monitoring data pipelines?\n",
    "\n",
    "### Answers\n",
    "\n",
    "`Airflow`. Apache Airflow is a fantastic tool for building, deploying, and monitoring data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105f416",
   "metadata": {},
   "source": [
    "## 9. Data pipeline architecture patterns\n",
    "\n",
    "### Description\n",
    "\n",
    "When building data pipelines, it's best to separate the files where functions are being defined from where they are being run.\n",
    "\n",
    "In this exercise, you'll practice importing components of a pipeline into memory before using these functions to run the pipeline end-to-end. The project takes the following format, where ``pipeline_utils`` stores the ``extract()``, ``transform()``, and ``load()`` functions that will be used run the pipeline.\n",
    "\n",
    "\n",
    "```bash\n",
    "> ls\n",
    " etl_pipeline.py\n",
    " pipeline_utils.py\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Import the ``extract``, ``transform``, and ``load`` functions from the ``pipeline_utils`` module.\n",
    "* Use the functions imported to run the data pipeline end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b283cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the extract, transform, and load functions from pipeline_utils\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "# Run the pipeline end to end by extracting, transforming and loading the data\n",
    "raw_tax_data = extract(raw_tax_data_csv_path)\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1d158",
   "metadata": {},
   "source": [
    "Great job! You've successfully imported data pipeline components from a pipeline_utils file, and ran the data pipeline end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef787f",
   "metadata": {},
   "source": [
    "## 10. Running a data pipeline end-to-end\n",
    "\n",
    "### Description\n",
    "\n",
    "It's important to monitor the performance of a pipeline when running in production. Earlier in the course, you explored tools such as exception handling and logging. In this last exercise, we'll practice running a pipeline end-to-end, while monitoring for exceptions and logging performance.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* From the ``pipeline_utils.py`` file, import the ``extract()``, ``transform()``, and ``load()`` functions.\n",
    "* Use the ``extract()``, ``transform()``, and ``load()`` functions to run the tax data pipeline end-to-end, within the ``try-except`` block.\n",
    "* Use the ``logging`` module to log an info-level success message if the pipeline executes as expected.\n",
    "* Create an error-level log if an exception occurs within the pipeline. Be sure to include the name of the exception in the log output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c2f1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Successfully extracted, transformed and loaded data.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Import extract, transform, and load functions from pipeline_utils\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "\t# Extract, transform, and load the tax data\n",
    "\traw_tax_data = extract(raw_tax_data_csv_path)\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "    \n",
    "\tlogging.info(\"Successfully extracted, transformed and loaded data.\")  # Log a success message.\n",
    "    \n",
    "except Exception as e:\n",
    "\tlogging.error(f\"Pipeline failed with error: {e}\")  # Log failure message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a82b3",
   "metadata": {},
   "source": [
    "Incredible! Using the logging module, try-except logic, and previously built ETL functionality, you've created an environment to run a pipeline end-to-end. Congrats!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
