{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7c3572",
   "metadata": {},
   "source": [
    "# 1. Introduction to PyTorch, a Deep Learning Library\n",
    "\n",
    "Self-driving cars, smartphones, search engines... Deep learning is now everywhere. Before you begin building complex models, you will become familiar with PyTorch, a deep learning framework. You will learn how to manipulate tensors, create PyTorch data structures, and build your first neural network in PyTorch with linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66452c",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae56673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b0861",
   "metadata": {},
   "source": [
    "## 1.2 User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No user variables here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e96ca5",
   "metadata": {},
   "source": [
    "# 2. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce564270",
   "metadata": {},
   "source": [
    "## 2.1 Getting started with PyTorch tensors\n",
    "\n",
    "### Description\n",
    "\n",
    "Tensors are PyTorch's core data structure and the foundation of deep learning. They're similar to NumPy arrays but have unique features.\n",
    "\n",
    "Here you have a Python list named ``temperatures`` containing daily readings from two weather stations. Try converting this into a tensor!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Begin by importing ``torch``.\n",
    "* Create a tensor from the Python list ``temperatures``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ffadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[72, 75, 78],\n",
      "        [70, 73, 76]])\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "temperatures = [[72, 75, 78], [70, 73, 76]]\n",
    "\n",
    "# Create a tensor from temperatures\n",
    "temp_tensor = torch.tensor(temperatures)\n",
    "\n",
    "print(temp_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b221cdf",
   "metadata": {},
   "source": [
    "### Analogy of Tensors vs NumPy Array to a restaurant\n",
    "\n",
    "* NumPy array is like a plain, traditional buffet tables\n",
    "    * Mutable - Modify the dishes directly (add/remove/change the ingredients)\n",
    "    * Works well on a single table (CPU), but is not suitable for GPUs\n",
    "    * Does not leverage special equipments (like a kitchen with dedicated chef or special tools)\n",
    "* Tensors are like high-tech gourmet kitchen with advanced appliances\n",
    "    * Immutable\n",
    "    * Can handle larger, more complex dishes very efficiently, especially if its connected to a faster stove (GPU or TPUI (Tensor Processing Unit by Google))\n",
    "    * Multiple tasks/dishes in parallel, supports special cooking techniques (automatic differentiation) used by pro chefs (deep learning models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469dec7",
   "metadata": {},
   "source": [
    "### Compare memory differences between NumPy arrays vs Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb453aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920d42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List: 365176 | NumPy: 80000 | Tensor: 80000\n"
     ]
    }
   ],
   "source": [
    "test_list = [i for i in range(10000)]\n",
    "test_numpy_arr = np.array(test_list)\n",
    "test_torch_tensor = torch.tensor(test_list)\n",
    "\n",
    "list_size = sys.getsizeof(test_list) + sum(sys.getsizeof(item) for item in test_list)\n",
    "numpy_size = test_numpy_arr.nbytes\n",
    "torch_size = test_torch_tensor.element_size() * test_torch_tensor.nelement()\n",
    "\n",
    "print(f\"List: {list_size} | NumPy: {numpy_size} | Tensor: {torch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feae8f3",
   "metadata": {},
   "source": [
    "The memory difference mentioned earlier refers to the entire object and runtime environment, not just the raw data buffer:\n",
    "\n",
    "* Tensors have additional metadata, state, and management overhead related to GPU support, autograd (automatic differentiation), and efficient memory handling for deep learning.\n",
    "* Tensors can also allocate memory on GPUs, requiring different memory bookkeeping beyond just the data buffer.\n",
    "* NumPy arrays are simpler, focused on CPU memory only, with less overhead but no GPU or autograd capabilities.\n",
    "\n",
    "So, when checking just the raw data size, they look the same. The actual memory footprint difference arises from tensor-specific features and how they manage memory beyond just storing data. That overhead is not included in simple ``.nbytes`` or ``.element_size()`` * ``nelement()`` calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92a621",
   "metadata": {},
   "source": [
    "## 2.2 Checking and adding tensors\n",
    "\n",
    "### Description\n",
    "\n",
    "While collecting temperature data, you notice the readings are off by two degrees. Add two degrees to the ``temperatures`` tensor after verifying its shape and data type with ``torch`` to ensure compatibility with the ``adjustment`` tensor.\n",
    "\n",
    "The ``torch`` library and the ``temperatures`` tensor are loaded for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Display the shape of the ``adjustment`` tensor.\n",
    "* Display the data type of the ``adjustment`` tensor.\n",
    "* Add the ``temperatures`` and ``adjustment`` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8878303",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ea8dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjustment shape: torch.Size([2, 3])\n",
      "Adjustment type: torch.int64\n",
      "Temperatures shape: torch.Size([2, 3])\n",
      "Temperatures type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Display the shape of the adjustment tensor\n",
    "print(\"Adjustment shape:\", adjustment.shape)\n",
    "\n",
    "# Display the type of the adjustment tensor\n",
    "print(\"Adjustment type:\", adjustment.dtype)\n",
    "\n",
    "print(\"Temperatures shape:\", temperatures.shape)\n",
    "print(\"Temperatures type:\", temperatures.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8cc17cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected temperatures: tensor([[74, 77, 80],\n",
      "        [72, 75, 78]])\n"
     ]
    }
   ],
   "source": [
    "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
    "\n",
    "# Add the temperatures and adjustment tensors\n",
    "corrected_temperatures = temperatures + adjustment\n",
    "print(\"Corrected temperatures:\", corrected_temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3355e7a9",
   "metadata": {},
   "source": [
    "## 2.3 Linear layer network\n",
    "\n",
    "### Description\n",
    "\n",
    "Neural networks often contain many layers, but most of them are linear layers. Understanding a single linear layer helps you grasp how they work before adding complexity.\n",
    "\n",
    "Apply a linear layer to an input tensor and observe the output.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create a ``Linear`` layer that takes 3 features as input and returns 2 outputs.\n",
    "* Pass ``input_tensor`` through the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cc2ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1443,  0.3594]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "\n",
    "# Create a Linear layer\n",
    "linear_layer = nn.Linear(\n",
    "                         in_features=3, \n",
    "                         out_features=2\n",
    "                         )\n",
    "\n",
    "# Pass input_tensor through the linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54fdcd",
   "metadata": {},
   "source": [
    "### Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680803e1",
   "metadata": {},
   "source": [
    "![Linear Neural Network Example](../images/nn_linear_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e581b07",
   "metadata": {},
   "source": [
    "## 2.4 Quiz: Understanding weights\n",
    "\n",
    "### Description\n",
    "\n",
    "In a linear model, weights and biases play a crucial role in determining how inputs are transformed into outputs. Understanding their function is key to building effective neural networks. Now, let's test your understanding!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "What is the role of weights in a neural network?\n",
    "\n",
    "### Answer\n",
    "\n",
    "Weights determine how much influence each input has on the neuron's output. Weights adjust the contribution of each input feature, allowing the network to learn patterns and make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f11470",
   "metadata": {},
   "source": [
    "## 2.5 Your first neural network\n",
    "\n",
    "### Description\n",
    "\n",
    "It's time for you to implement a small neural network containing two linear layers in sequence.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Add a container for stacking layers in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f6d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9240]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Create a container for stacking linear layers\n",
    "model = nn.Sequential(nn.Linear(8, 4),\n",
    "                nn.Linear(4, 1)\n",
    "                )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32361fd1",
   "metadata": {},
   "source": [
    "## 2.6 Stacking linear layers\n",
    "\n",
    "### Description\n",
    "\n",
    "Nice work building your first network with two linear layers. Let's stack some more layers. Remember that a neural network can have as many hidden layers as we want, provided the inputs and outputs line up.\n",
    "\n",
    "This network is designed to ingest the following input:\n",
    "\n",
    "```py\n",
    "input_tensor = torch.Tensor(\n",
    "    [[2, 7, 9, 5, 3]]\n",
    "    )\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Reorder the items provided to create a neural network with three hidden layers and an output of size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f5d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[2, 7, 9, 5, 3]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 20),\n",
    "    nn.Linear(20, 14),\n",
    "    nn.Linear(14, 3),\n",
    "    nn.Linear(3, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111c06a",
   "metadata": {},
   "source": [
    "## 2.7 Counting the number of parameters\n",
    "\n",
    "### Description\n",
    "\n",
    "Deep learning models are famous for having a lot of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.\n",
    "\n",
    "In this exercise, you'll first calculate the number of parameters manually. Then, you'll verify your result using the ``.numel()`` method.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Q1. Manually calculate the number of parameters of the model below. How many does it have? Use the console as a calculator.\n",
    "```py\n",
    "model = nn.Sequential(nn.Linear(9, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "```\n",
    "* Use ``.numel()`` to confirm your manual calculation by iterating through the model's parameters to updating the ``total`` variable.\n",
    "\n",
    "### Answer: \n",
    "\n",
    "* Answer1: `53`.\n",
    "    * First Layer (9,4):\n",
    "        * Weights: 9 inputs x 4 outputs = 36 parameters\n",
    "        * Biases: 4 parameters (same as outputs)\n",
    "        * First Layer paraneters: 40+4 = 40 parameters\n",
    "    * Second Layer (4,2):\n",
    "        * Weights: 4 inputs x 2 outputs = 8 parameters\n",
    "        * Biases: 2 parameters (same as outputs)\n",
    "        * Second Layer paraneters: 8+2 = 10 parameters\n",
    "    * Third / Output Layer (2,1):\n",
    "        * Weights: 2 inputs x 1 output = 2 parameters\n",
    "        * Biases: 1 parameter (same as output)\n",
    "        * Output Layer paraneters: 2+1 = 3 parameters\n",
    "    * 40 + 10 + 3 = 53 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba35da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(9, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "for parameter in model.parameters():\n",
    "    total += parameter.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd13aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the model is 53\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(9, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for p in model.parameters():\n",
    "  total += p.numel()\n",
    "  \n",
    "print(f\"The number of parameters in the model is {total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
