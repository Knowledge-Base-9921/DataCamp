{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f903c6f0",
   "metadata": {},
   "source": [
    "# 1. Neural Network Architecture and Hyperparameters\n",
    "\n",
    "To train a neural network in PyTorch, you will first need to understand additional components, such as activation and loss functions. You will then realize that training a network requires minimizing that loss function, which is done by calculating gradients. You will learn how to use these gradients to update your model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71188eb",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5754f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa5fe3a",
   "metadata": {},
   "source": [
    "## 1.2 User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba42dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No user variables here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a3f86",
   "metadata": {},
   "source": [
    "# 2. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94955585",
   "metadata": {},
   "source": [
    "## 2.1 Quiz: Activate your understanding!\n",
    "\n",
    "### Description \n",
    "\n",
    "Neural networks are a core component of deep learning models. They can power so much in your daily life, from language translation apps to the cameras on your smartphone.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Which of the following statements about neural networks is True?\n",
    "\n",
    "### Answers\n",
    "\n",
    "* A neural network with a single linear layer followed by a sigmoid activation is similar to a logistic regression model.\n",
    "\n",
    "An activation function in a neural network is a mathematical function applied to the output of a neuron or a layer. Its main role is to introduce non-linearity into the model, enabling the network to learn and represent complex patterns in data. Without an activation function, all layers would essentially behave as linear transformations, and no matter how many layers the network has, it would reduce to a single linear function, limiting the model's ability to solve complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de86db",
   "metadata": {},
   "source": [
    "## 2.2 The sigmoid and softmax functions\n",
    "\n",
    "### Description\n",
    "\n",
    "The sigmoid and softmax functions are key activation functions in deep learning, often used as the final step in a neural network.\n",
    "\n",
    "* Sigmoid is for binary classification\n",
    "    ![Sigmoid function](../images/sigmoid_function.png)\n",
    "\n",
    "* Softmax is for multi-class classification\n",
    "    ![Softmax function](../images/softmax_function.png)\n",
    "\n",
    "![Classifications for Activation](../images/sigmoid_and_softmax.png)\n",
    "\n",
    "Given a pre-activation output tensor from a network, apply the appropriate activation function to obtain the final output.\n",
    "\n",
    "``torch.nn`` has already been imported as ``nn``.\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Softmax Activation as a special case of Sigmoid Function\n",
    "    * ![Softmax as sigmoid function](../images/softmax_as_sigmoid.png)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create a sigmoid function and apply it on ``input_tensor`` to generate a probability for a binary classification task.\n",
    "* Create a softmax function and apply it on ``input_tensor`` to generate a probability for a multi-class classification task.\n",
    "* Softmax function for two classes produces the same result as sigmoid function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed43f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9168]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[2.4]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9409ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax()\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa88fbf",
   "metadata": {},
   "source": [
    "### Practice: Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a0287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47fea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max for numerical stability\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb5fab",
   "metadata": {},
   "source": [
    "## 2.3 Building a binary classifier in PyTorch\n",
    "\n",
    "### Description\n",
    "\n",
    "Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n",
    "\n",
    "Practice building this small network and interpreting the output of the classifier.\n",
    "\n",
    "### Notes\n",
    "\n",
    "![Binary_Classifier_8_1](../images/binary_classifier_8_1.png)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create a neural network that takes a 1x8 tensor as input and outputs a single value for binary classification.\n",
    "* Pass the output of the linear layer to a sigmoid to produce a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db943783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8050]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8,1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output) # tensor([[0.0127]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462391b2",
   "metadata": {},
   "source": [
    "### Practice: Manual Code of `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edeb0602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear output: 0.7500000000000004\n",
      "Sigmoid output: 0.6791786991753931\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Input data\n",
    "inputs = [3, 4, 6, 2, 3, 6, 8, 9]  # same as input_tensor row\n",
    "\n",
    "# Step 2: Example weights and bias (from nn.Linear). These would be learned; here we pick arbitrary ones.\n",
    "weights = [0.1, -0.2, 0.05, 0.3, -0.15, 0.4, -0.25, 0.1]  # 8 weights\n",
    "bias = -0.5\n",
    "\n",
    "# Step 3: Compute the linear combination (dot product + bias)\n",
    "linear_output = sum(w * x for w, x in zip(weights, inputs)) + bias\n",
    "print(\"Linear output:\", linear_output)\n",
    "\n",
    "# Step 4: Apply sigmoid manually\n",
    "sigmoid_output = 1 / (1 + np.exp(-linear_output))\n",
    "print(\"Sigmoid output:\", sigmoid_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e17ed1",
   "metadata": {},
   "source": [
    "### Quiz:\n",
    "\n",
    "Which of the following is false about the output returned by your binary classifier?\n",
    "\n",
    "* We can use a threshold of 0.5 to determine if the output belongs to one class or the other.\n",
    "* It can return any float value. [x]\n",
    "* It is produced from an untrained model so it is not yet meaningful.\n",
    "* The sigmoid function transforms the values of the input without changing its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d2633",
   "metadata": {},
   "source": [
    "## 2.4 From regression to multi-class classification\n",
    "\n",
    "### Description\n",
    "\n",
    "The models you have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n",
    "\n",
    "Start building a model for regression, and then tweak the model to perform a multi-class classification.\n",
    "\n",
    "### Instruction\n",
    "\n",
    "* Create a 4-layer linear network that takes 11 input features from ``input_tensor`` and produces a single regression output.\n",
    "* Update the network provided to perform a multi-class classification with four outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ea6b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2031, 0.3338, 0.0875, 0.3755]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4),\n",
    "  nn.Softmax()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf74a0",
   "metadata": {},
   "source": [
    "## 2.5 Creating one-hot encoded labels\n",
    "\n",
    "### Description\n",
    "\n",
    "One-hot encoding converts a single integer label into a vector with N elements, where N is the number of classes. This vector contains zeros and a one at the correct position.\n",
    "\n",
    "In this exercise, you'll manually create a one-hot encoded vector for y, and then use PyTorch to simplify the process. Your dataset has three classes (0, 1, 2).\n",
    "\n",
    "``numpy (np)``, '``torch.nn.functional (F)``', and ``torch`` are already imported for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Manually one-hot encode the ground truth label ``y`` using the provided NumPy array and save it as ``one_hot_numpy``.\n",
    "* Use PyTorch to one-hot encode ``y`` and save it as ``one_hot_pytorch``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b6b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot vector using NumPy: [0 1 0]\n",
      "One-hot vector using PyTorch: tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes=3)\n",
    "\n",
    "print(\"One-hot vector using NumPy:\", one_hot_numpy)\n",
    "print(\"One-hot vector using PyTorch:\", one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0556b76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot vector using PyTorch: tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "y = 2\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes=3)\n",
    "\n",
    "print(\"One-hot vector using PyTorch:\", one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6c651",
   "metadata": {},
   "source": [
    "* Class `0`: [1,0,0]\n",
    "* Class `1`: [0,1,0]\n",
    "* Class `2`: [0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76e315",
   "metadata": {},
   "source": [
    "## 2.6 Calculating cross entropy loss\n",
    "\n",
    "### Description\n",
    "\n",
    "Cross-entropy loss is a widely used method to measure classification loss. In this exercise, you’ll calculate cross-entropy loss in PyTorch using:\n",
    "\n",
    "* ``y``: the ground truth label.\n",
    "* ``scores``: a vector of predictions before softmax.\n",
    "\n",
    "Loss functions help neural networks learn by measuring prediction errors. Create a one-hot encoded vector for ``y``, define the cross-entropy loss function, and compute the loss using ``scores`` and the encoded label. The result will be a single float representing the sample's loss.\n",
    "\n",
    "``torch``, ``CrossEntropyLoss``, and ``torch.nn.functional`` as ``F`` have already been imported for you.\n",
    "\n",
    "### Notes\n",
    "\n",
    "* CRE measures how well the predicted probability distribution of a model matches the true distribution of the labels.\n",
    "* It is often used for binary classification (called Binary Cross-Entropy Loss) and multi-class classification (Categorical Cross-Entropy Loss).\n",
    "* It uses a logarithmic function, which is why it’s sometimes called log loss.\n",
    "* Formula for Binary Cross Entropy:\n",
    "\n",
    "![BCE](../images/bce.png)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create the one-hot encoded vector of the ground truth label ``y``, with 4 features (one for each class), and assign it to ``one_hot_label``.\n",
    "* Create the cross entropy loss function and store it as ``criterion``.\n",
    "* Calculate the cross entropy loss using the ``one_hot_label`` vector and the ``scores`` vector, by calling the ``loss_function`` you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f44dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=4)\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda52c3",
   "metadata": {},
   "source": [
    "## 2.7 Accessing the model parameters\n",
    "\n",
    "### Description\n",
    "\n",
    "A PyTorch model created with the ``nn.Sequential()`` is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Access the ``weight`` parameter of the first linear layer.\n",
    "* Access the ``bias`` parameter of the second linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02de3022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[ 0.1599,  0.1900,  0.1030, -0.1427, -0.1252, -0.2235,  0.0390,  0.1285,\n",
      "          0.0842, -0.0871,  0.0149,  0.1775,  0.2247,  0.0153, -0.2219,  0.0035],\n",
      "        [ 0.2420, -0.1418, -0.1945, -0.1317, -0.0870,  0.1383,  0.2400,  0.1870,\n",
      "          0.2184,  0.1664,  0.0349,  0.1237, -0.2163,  0.1853,  0.1931, -0.1957],\n",
      "        [ 0.1005, -0.2228,  0.2209, -0.0236,  0.0581,  0.0340, -0.0561,  0.1102,\n",
      "         -0.1637, -0.0369,  0.1283,  0.1468,  0.0893,  0.2418,  0.1864, -0.2202],\n",
      "        [-0.0409, -0.0627, -0.1636, -0.1712,  0.2033, -0.0048, -0.2290,  0.0282,\n",
      "          0.1778,  0.2003, -0.1220, -0.0683, -0.0909, -0.1600,  0.0121,  0.1201],\n",
      "        [ 0.0054, -0.1840, -0.0604,  0.2043, -0.1347, -0.1650,  0.0729, -0.1691,\n",
      "         -0.0646,  0.1013,  0.2091, -0.0743,  0.0278,  0.1342,  0.2025,  0.0538],\n",
      "        [ 0.1109, -0.1858, -0.1443,  0.2490, -0.2001,  0.1719,  0.1588,  0.0625,\n",
      "         -0.1041,  0.2239,  0.0731,  0.1850, -0.1722, -0.0436, -0.2424,  0.1627],\n",
      "        [-0.1747, -0.0661,  0.2450,  0.2035, -0.1209,  0.2138,  0.1895, -0.0145,\n",
      "         -0.2344, -0.1592,  0.0378,  0.0447,  0.2023, -0.2361, -0.0049, -0.0397],\n",
      "        [-0.0088, -0.0639, -0.0767, -0.2271, -0.0611, -0.0779,  0.1294,  0.1886,\n",
      "          0.1566, -0.1247, -0.2272,  0.1684, -0.1423, -0.1874, -0.2340,  0.2193]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([0.0852, 0.1529], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Linear(8, 2)\n",
    "                     )\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "print(\"Weight of the first layer:\", weight_0)\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[1].bias\n",
    "print(\"Bias of the second layer:\", bias_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b34e9",
   "metadata": {},
   "source": [
    "## 2.8 Updating the weights manually\n",
    "\n",
    "### Description\n",
    "\n",
    "Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. While PyTorch automates this, practicing it manually helps you build intuition for how models learn and adjust. This understanding will be valuable when debugging or fine-tuning neural networks.\n",
    "\n",
    "A neural network of three layers has been created and stored as the ``model`` variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, ``lr``, has been chosen to scale the gradients when performing the update.\n",
    "\n",
    "### Notes\n",
    "\n",
    "![Gradients](../images/grads.png)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Create the gradient variables by accessing the local gradients of each weight tensor.\n",
    "* Update the weights using the gradients scaled by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.47962456941604614\n",
      "Output probabilities: [[0.3809843  0.61901575]]\n",
      "Epoch 2, Loss: 0.4686005413532257\n",
      "Output probabilities: [[0.37412247 0.62587756]]\n",
      "Epoch 3, Loss: 0.4576696753501892\n",
      "Output probabilities: [[0.36724356 0.6327565 ]]\n",
      "Epoch 4, Loss: 0.44683101773262024\n",
      "Output probabilities: [[0.36034802 0.639652  ]]\n",
      "Epoch 5, Loss: 0.4360750615596771\n",
      "Output probabilities: [[0.3534308 0.6465692]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(16,8),\n",
    "  nn.Linear(8,4),\n",
    "  nn.Linear(4,2)\n",
    ")\n",
    "\n",
    "input_tensor = torch.randn(1, 16) # Sample to get grads, else it will be blank\n",
    "output_tensor = torch.tensor([1]) # Sample output to get grads, else it will be blank\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "for epoch in range(5):  # train for 5 epochs for example\n",
    "    optimizer.zero_grad()             # Reset gradients\n",
    "    output = model(input_tensor)      # Forward pass\n",
    "    loss = criterion(output, output_tensor)  # Compute loss\n",
    "    loss.backward()                   # Backward pass\n",
    "    optimizer.step()                  # Update weights\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "    print(\"Output probabilities:\", nn.functional.softmax(output, dim=1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9120e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4681599",
   "metadata": {},
   "source": [
    "## 2.9 Using the PyTorch optimizer\n",
    "\n",
    "### Description\n",
    "\n",
    "Earlier, you manually updated the weight of a network, gaining insight into how training works behind the scenes. However, this method isn’t scalable for deep networks with many layers.\n",
    "\n",
    "Thankfully, PyTorch provides the SGD optimizer, which automates this process efficiently in just a few lines of code. Now, you’ll complete the training loop by updating the weights using a PyTorch optimizer.\n",
    "\n",
    "A neural network has been created and provided as the ``model`` variable. This model was used to run a forward pass and create the tensor of predictions ``pred``. The one-hot encoded tensor is named ``target`` and the cross entropy loss function is stored as ``criterion``.\n",
    "\n",
    "``torch.optim`` as ``optim``, and ``torch.nn`` as ``nn`` have already been loaded for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Use ``optim`` to create an ``SGD`` optimizer with a learning rate of your choice (must be less than one) for the model provided.\n",
    "* Update the model's parameters using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54687708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5398,  0.0948]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(input_tensor)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01936a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([[1.0, 0.0]])\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cff80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
